03/20/25

We'll be using our grpc implementation. To start, we have the following general outline:

- The client will connect to the leader address
- The leader will connect to 4 followers, since follows 2(2) + 1
- When the client interacts with the leader, the leader has to update each of the other servers
    - We anticipate this to be the hardest part, since (at this point) we don't know how to implement this yet
- If a follower fails, continue as usual (the leader only needs majority to continue, at least 3)
- If the leader fails, we need to elect a follower
    - Reading online, this can be done using something called Raft
    - If a certain amount of time has passed and there is no message from the leader, a follower nominates itself and asks for majority votes

It might be useful to have a RaftNode for servers to talk to each other, so ChatResponse is client-server, and Raft is server-server

Ok, so first, we need to rework the "online clients" system.

03/22/25
After experimenting a lot and reading online, we are going to take it step by step.

First: Making new databases for each replica (5 total for 2 fault tolerant)
UNIT TESTS CREATED:
    TestDatabaseSetup

First (possibly easiest step), we just want to have multiple servers connected.
The client will still only connect to one server (the assumed leader)

03/23/25
Added raft.proto (for real this time) that has one service and 3 main functions:
    One for voting (between raft)
    One for entries (heartbeats, replication)
    One for getleader (for client to get leader in case of leader crash)

Servers now wait until all other servers are online before proceeding.

03/24/25

Changed chat.proto today to include a new action "Connect" so that when clients connect to leader,
the leader can know they are online.
This should help deal with the issue of leader dying and connected users being lost.

For now, we can't write unit test just yet, since there are still some bugs in the leader election.
Once we get the leader election up and running, we can begin writing some unit tests.

Ok, election now functions properly. We had to add a timer to the code such that each server will wait a certain amount of time
before trying to run for candidate. Once a server is a candidate, it sends vote requests from everyone and continues if it has 3 votes.

Client also now requests new leader via iteration after server fails. As of right now, it retries 6 times (to allow time to elect new leader).
This seems to be working well, but databases are not consistent yet. Need to add a new file called "replica_helpers" specifically for this action,
since we just want to change the databases and nothing else. A lot of this code will be based on the Chat service, but it will be different in how it
interacts with the client (that is, it won't be interacting with the client at all!)

The client also shuts down connections if it fails to find any server after 6 retries (just so it is not perpetually trying to connect to a socket).

03/25/25
Ok, should now be prepared for testing. Here's the schematic:
- Once servers are all connected, an election is run
- Leader is selected via majority vote
- Every 0.1 seconds, leader sends heartbeat containing logs to all followers
- If there is a difference between the last commit and previous entries, followers replicate actions then vote to commit
- Commit succeeds with majority vote
- When client connects, it finds leader and connects
- Any request handled by leader is appended to logs and sent in next heartbeat
- If leader goes down, client detects server is down and waits for next election to finish before getting new leader
    - The client needs to send 2 connect requests, because (the way threads work) it needs to join the previous thread and crash it via a yield statement

Now that the process is stable, we can begin testing each part via unit tests.

Added tests to make sure function functionality works for all servers and replicas.
Also added new test case to ensure the system works with 0 crashes, 1 crash, and 2 crashes,
but then doesn't work with 3 or more crashes.
Also fixed bug where requests would be overwritten in the "handle_requests" by the leader by
creating copys of the variables.

Now, we work on getting it to work across devices. From what we remember, there should be a public IP address command (from lecture)
that we can run.